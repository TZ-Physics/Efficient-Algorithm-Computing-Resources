# Efficient-Algorithm-Computing-Resources

## Deep Learning

### Transformer

**Efficient transformers: A survey.**<br>
*Y Tay, M Dehghani, D Bahri, D Metzler.*<br>
ACM Computing Surveys, 2022.
[[Paper](https://dl.acm.org/doi/pdf/10.1145/3530811)]

**A survey on efficient training of transformers.**<br>
*B Zhuang, J Liu, Z Pan, H He, Y Weng, C Shen.*<br>
arXiv:2302.01107, 2023.
[[Paper](https://arxiv.org/pdf/2302.01107)]

**Full stack optimization of transformer inference: a survey.**<br>
*S Kim, C Hooper, T Wattanawong, M Kang, R Yan, H Genc, G Dinh, Q Huang, K Keutzer, et al.*<br>
arXiv:2302.14017, 2023.
[[Paper](https://arxiv.org/pdf/2302.14017)]

### LLM

**Smoothquant: Accurate and efficient post-training quantization for large language models.**<br>
*G Xiao, J Lin, M Seznec, H Wu, J Demouth, S Han.*<br>
International Conference on Machine Learning, 2023.
[[Paper](https://proceedings.mlr.press/v202/xiao23c/xiao23c.pdf)]
[[Github](https://github.com/mit-han-lab/smoothquant)]

**DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads.**<br>
*G Xiao, J Tang, J Zuo, J Guo, S Yang, H Tang, Y Fu, S Han.*<br>
arXiv:2410.10819, 2024.
[[Paper](https://arxiv.org/pdf/2410.10819)]
[[Github](https://github.com/mit-han-lab/duo-attention)]

**Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference.**<br>
*W Luk, KFC Yiu, R Li, K Mishchenko, SI Venieris, H Fan.*<br>
arxiv:2405.18628, 2024.
[[Paper](https://arxiv.org/pdf/2405.18628)]
[[Github](https://github.com/hmarkc/parallel-prompt-decoding)]

**QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving.**<br>
*Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Ming-Yu Liu, Kai Li, Song Han.*<br>
ArXiv, 2024.
[[Paper](https://arxiv.org/pdf/2405.04532)]
[[Github](https://hanlab.mit.edu/projects/qserve)]

**Efficient streaming language models with attention sinks.**<br>
*G Xiao, Y Tian, B Chen, S Han, M Lewis.*<br>
ICLR, 2024.
[[Paper](https://arxiv.org/pdf/2309.17453)]
[[Github](https://github.com/mit-han-lab/streaming-llm)]

**AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration.**<br>
*Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, Song Han.*<br>
MLSys, 2024.
[[Paper](https://arxiv.org/pdf/2306.00978)]
[[Github](https://github.com/mit-han-lab/llm-awq)]

### VLM

**Efficientvlm: Fast and accurate vision-language models via knowledge distillation and modal-adaptive pruning.**<br>
*T Wang, W Zhou, Y Zeng, X Zhang.*<br>
arxiv:2210.07795, 2022.
[[Paper](https://arxiv.org/pdf/2210.07795)]
[[Github](https://github.com/swaggy-TN/EfficientVLM)]

**MobileVLM: A Fast, Strong and Open Vision Language Assistant for Mobile Devices.**<br>
*Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, et al.*<br>
arXiv, 2023.
[[Paper](https://arxiv.org/pdf/2312.16886v2)]
[[Github](https://github.com/Meituan-AutoML/MobileVLM)]

**MobileVLM V2: Faster and Stronger Baseline for Vision Language Model.**<br>
*X Chu, L Qiao, X Zhang, S Xu, F Wei, Y Yang, et al.*<br>
ArXiv, 2024.
[[Paper](https://arxiv.org/pdf/2402.03766)]
[[Github](https://github.com/Meituan-AutoML/MobileVLM)]

### Diffusion Models

**Snapfusion: Text-to-image diffusion model on mobile devices within two seconds.**<br>
*Y Li, H Wang, Q Jin, J Hu, P Chemerys, Y Fu, Y Wang, S Tulyakov, J Ren.*<br>
Advances in Neural Information Processing Systems, 2024.
[[Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/41bcc9d3bddd9c90e1f44b29e26d97ff-Paper-Conference.pdf)]
[[Github](https://snap-research.github.io/SnapFusion)]

**Deep Compression Autoencoder for Efficient High-Resolution Diffusion Models.**<br>
*J Chen, H Cai, J Chen, E Xie, S Yang, H Tang, M Li, Y Lu, S Han.*<br>
arXiv:2410.10733, 2024.
[[Paper](https://arxiv.org/pdf/2410.10733)]
[[Github](https://github.com/mit-han-lab/efficientvit)]

**Deepcache: Accelerating diffusion models for free.**<br>
*X Ma, G Fang, X Wang.*<br>
CVPR, 2024.
[[Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_DeepCache_Accelerating_Diffusion_Models_for_Free_CVPR_2024_paper.pdf)]
[[Github](https://github.com/horseee/DeepCache)]

**DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models.**<br>
*Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Ming-Yu Liu, Kai Li, Song Han.*<br>
CVPR, 2024.
[[Paper](https://arxiv.org/pdf/2402.19481)]
[[Github](https://github.com/mit-han-lab/distrifuser)]

**SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformers.**<br>
*E Xie, J Chen, J Chen, H Cai, Y Lin, Z Zhang, M Li, Y Lu, S Han.*<br>
arXiv:2410.10629, 2024.
[[Paper](https://arxiv.org/pdf/2410.10629)]
[[Github](https://nvlabs.github.io/Sana/)]

## TinyML

### Review

**Tiny Machine Learning: Progress and Futures.**<br>
*J Lin, L Zhu, WM Chen, WC Wang, et al.*<br>
IEEE Circuits and Systems Magazine 23 (3), 8-34, 2023.
[[Paper](https://arxiv.org/pdf/2403.19076)]

**Intelligence at the extreme edge: A survey on reformable tinyml.**<br>
*V Rajapakse, I Karunanayake, N Ahmed.*<br>
ACM Computing Surveys, 2023.
[[Paper](https://arxiv.org/pdf/2204.00827)]

### Training

**PockEngine: Sparse and Efficient Fine-tuning in a Pocket.**<br>
*L Zhu, L Hu, J Lin, WM Chen, WC Wang, C Gan, S Han.*<br>
MICRO, 2023.
[[Paper](https://dl.acm.org/doi/pdf/10.1145/3613424.3614307)]

**On-device training under 256kb memory.**<br>
*J Lin, L Zhu, WM Chen, WC Wang, et al.*<br>
NeurIPS, 2022.
[[Paper](https://proceedings.neurips.cc/paper_files/paper/2022/file/90c56c77c6df45fc8e556a096b7a2b2e-Paper-Conference.pdf)]

**Training Machine Learning models at the Edge: A Survey.**<br>
*AR Khouas, MR Bouadjenek, H Hacid, et al.*<br>
ArXiv, 2023.
[[Paper](https://arxiv.org/pdf/2403.02619)]

### MCU

**Mcunet: Tiny deep learning on iot devices.**<br>
*J Lin, WM Chen, Y Lin, C Gan, S Han.*<br>
Advances in Neural Information Processing Systems, 2020.
[[Paper](https://proceedings.neurips.cc/paper/2020/file/86c51678350f656dcc7f490a43946ee5-Paper.pdf)]
[[Github](https://hanlab.mit.edu/projects/tinyml)]

**Mcunetv2: Memory-efficient patch-based inference for tiny deep learning.**<br>
*J Lin, WM Chen, H Cai, C Gan, S Han.*<br>
arXiv:2110.15352, 2021.
[[Paper](https://arxiv.org/pdf/2110.15352)]
[[Github](https://hanlab.mit.edu/projects/tinyml)]

**Deepcache: Accelerating diffusion models for free.**<br>
*X Ma, G Fang, X Wang, et al.*<br>
CVPR, 2024.
[[Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_DeepCache_Accelerating_Diffusion_Models_for_Free_CVPR_2024_paper.pdf)]
[[Github](https://github.com/horseee/DeepCache)]
